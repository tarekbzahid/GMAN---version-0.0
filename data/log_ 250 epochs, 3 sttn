K=8, L=3, SE_file='./data/SE(LAS-28).txt', batch_size=32, d=8, decay_epoch=20, learning_rate=0.01, log_file='./data/log', max_epoch=250, model_file='./data/GMAN.pkl', num_his=12, num_pred=12, patience=100, test_ratio=0.2, time_slot=5, traffic_file='./data/speed2018modified.h5', train_ratio=0.7, val_ratio=0.1
loading data...
trainX: torch.Size([24504, 12, 28])		 trainY: torch.Size([24504, 12, 28])
valX:   torch.Size([3481, 12, 28])		valY:   torch.Size([3481, 12, 28])
testX:   torch.Size([6985, 12, 28])		testY:   torch.Size([6985, 12, 28])
mean:   61.3232		std:   14.4641
data loaded!
compiling model...
trainable parameters: 513,795
**** training model ****
2023-03-29 15:03:06 | epoch: 0001/250, training time: 973.6s, inference time: 45.6s
train loss: 51.2910, val_loss: 112.4467
val loss decrease from inf to 112.4467, saving model to ./data/GMAN.pkl
2023-03-29 15:19:18 | epoch: 0002/250, training time: 930.9s, inference time: 41.5s
train loss: 41.6556, val_loss: 256.9774
2023-03-29 15:35:29 | epoch: 0003/250, training time: 928.3s, inference time: 41.7s
train loss: 38.7504, val_loss: 316.5085
2023-03-29 15:51:34 | epoch: 0004/250, training time: 923.8s, inference time: 41.4s
train loss: 37.6142, val_loss: 120.5389
2023-03-29 16:07:44 | epoch: 0005/250, training time: 928.4s, inference time: 41.9s
train loss: 36.6117, val_loss: 90.5521
val loss decrease from 112.4467 to 90.5521, saving model to ./data/GMAN.pkl
2023-03-29 16:23:54 | epoch: 0006/250, training time: 929.0s, inference time: 40.8s
train loss: 36.3131, val_loss: 72.8094
val loss decrease from 90.5521 to 72.8094, saving model to ./data/GMAN.pkl
2023-03-29 16:40:09 | epoch: 0007/250, training time: 932.3s, inference time: 42.6s
train loss: 35.3952, val_loss: 175.3042
2023-03-29 16:55:53 | epoch: 0008/250, training time: 902.6s, inference time: 41.5s
train loss: 34.4398, val_loss: 107.8625
2023-03-29 17:13:21 | epoch: 0009/250, training time: 1005.4s, inference time: 43.0s
train loss: 33.7633, val_loss: 85.3729
2023-03-29 17:29:17 | epoch: 0010/250, training time: 913.6s, inference time: 42.0s
train loss: 33.7190, val_loss: 88.6635
2023-03-29 17:45:04 | epoch: 0011/250, training time: 904.8s, inference time: 41.9s
train loss: 33.6160, val_loss: 96.7099
2023-03-29 18:01:09 | epoch: 0012/250, training time: 922.5s, inference time: 42.9s
train loss: 32.3755, val_loss: 90.8567
2023-03-29 18:16:52 | epoch: 0013/250, training time: 899.6s, inference time: 42.9s
train loss: 31.8755, val_loss: 89.4015
2023-03-29 18:32:54 | epoch: 0014/250, training time: 919.8s, inference time: 42.7s
train loss: 31.6639, val_loss: 71.8895
val loss decrease from 72.8094 to 71.8895, saving model to ./data/GMAN.pkl
2023-03-29 18:48:52 | epoch: 0015/250, training time: 915.9s, inference time: 42.0s
train loss: 31.6449, val_loss: 56.4588
val loss decrease from 71.8895 to 56.4588, saving model to ./data/GMAN.pkl
2023-03-29 19:04:53 | epoch: 0016/250, training time: 919.4s, inference time: 41.9s
train loss: 31.0046, val_loss: 77.9715
2023-03-29 19:21:01 | epoch: 0017/250, training time: 925.6s, inference time: 42.0s
train loss: 30.1954, val_loss: 50.7828
val loss decrease from 56.4588 to 50.7828, saving model to ./data/GMAN.pkl
2023-03-29 19:37:13 | epoch: 0018/250, training time: 930.4s, inference time: 41.8s
train loss: 29.4680, val_loss: 82.2077
2023-03-29 19:53:11 | epoch: 0019/250, training time: 915.4s, inference time: 42.4s
train loss: 29.6775, val_loss: 75.7335
2023-03-29 20:09:06 | epoch: 0020/250, training time: 913.2s, inference time: 41.5s
train loss: 29.0679, val_loss: 70.0422
2023-03-29 20:25:08 | epoch: 0021/250, training time: 919.7s, inference time: 42.2s
train loss: 28.6228, val_loss: 75.4248
2023-03-29 20:42:24 | epoch: 0022/250, training time: 993.9s, inference time: 42.0s
train loss: 28.3946, val_loss: 46.2206
val loss decrease from 50.7828 to 46.2206, saving model to ./data/GMAN.pkl
2023-03-29 20:58:28 | epoch: 0023/250, training time: 918.7s, inference time: 45.0s
train loss: 28.1067, val_loss: 71.3578
2023-03-29 21:14:21 | epoch: 0024/250, training time: 910.5s, inference time: 42.6s
train loss: 27.5067, val_loss: 71.4868
2023-03-29 21:30:20 | epoch: 0025/250, training time: 918.0s, inference time: 41.1s
train loss: 27.5488, val_loss: 61.7242
2023-03-29 21:46:17 | epoch: 0026/250, training time: 916.2s, inference time: 41.3s
train loss: 27.0871, val_loss: 45.5826
val loss decrease from 46.2206 to 45.5826, saving model to ./data/GMAN.pkl
2023-03-29 22:02:04 | epoch: 0027/250, training time: 904.9s, inference time: 41.7s
train loss: 26.7279, val_loss: 49.7076
2023-03-29 22:18:03 | epoch: 0028/250, training time: 916.1s, inference time: 42.4s
train loss: 26.5052, val_loss: 45.5380
val loss decrease from 45.5826 to 45.5380, saving model to ./data/GMAN.pkl
2023-03-29 22:34:05 | epoch: 0029/250, training time: 920.6s, inference time: 41.7s
train loss: 26.6110, val_loss: 60.1326
2023-03-29 22:49:59 | epoch: 0030/250, training time: 912.1s, inference time: 41.7s
train loss: 26.8041, val_loss: 58.1563
2023-03-29 23:06:09 | epoch: 0031/250, training time: 929.5s, inference time: 40.3s
train loss: 26.3997, val_loss: 55.9661
2023-03-29 23:22:14 | epoch: 0032/250, training time: 923.4s, inference time: 42.2s
train loss: 26.1544, val_loss: 61.4822
2023-03-29 23:38:09 | epoch: 0033/250, training time: 913.6s, inference time: 41.4s
train loss: 26.4623, val_loss: 46.0294
2023-03-29 23:54:19 | epoch: 0034/250, training time: 926.0s, inference time: 43.7s
train loss: 26.2013, val_loss: 44.7320
val loss decrease from 45.5380 to 44.7320, saving model to ./data/GMAN.pkl
2023-03-30 00:10:22 | epoch: 0035/250, training time: 921.5s, inference time: 41.2s
train loss: 26.1707, val_loss: 50.6797
2023-03-30 00:26:10 | epoch: 0036/250, training time: 906.6s, inference time: 41.2s
train loss: 26.0211, val_loss: 64.0054
2023-03-30 00:42:15 | epoch: 0037/250, training time: 923.0s, inference time: 42.5s
train loss: 25.7015, val_loss: 53.7125
2023-03-30 00:58:14 | epoch: 0038/250, training time: 916.5s, inference time: 41.9s
train loss: 25.6199, val_loss: 49.7063
2023-03-30 01:14:08 | epoch: 0039/250, training time: 911.9s, inference time: 41.9s
train loss: 25.0863, val_loss: 46.5252
2023-03-30 01:30:16 | epoch: 0040/250, training time: 926.9s, inference time: 41.7s
train loss: 25.3020, val_loss: 47.1134
2023-03-30 01:46:18 | epoch: 0041/250, training time: 919.9s, inference time: 42.1s
train loss: 25.1335, val_loss: 43.2191
val loss decrease from 44.7320 to 43.2191, saving model to ./data/GMAN.pkl
2023-03-30 02:02:23 | epoch: 0042/250, training time: 921.3s, inference time: 43.3s
train loss: 24.8138, val_loss: 49.9688
2023-03-30 02:18:24 | epoch: 0043/250, training time: 919.5s, inference time: 41.7s
train loss: 24.8214, val_loss: 45.2278
2023-03-30 02:34:30 | epoch: 0044/250, training time: 922.7s, inference time: 43.1s
train loss: 24.7638, val_loss: 51.8634
2023-03-30 02:50:28 | epoch: 0045/250, training time: 916.9s, inference time: 41.6s
train loss: 24.8750, val_loss: 61.1833
2023-03-30 03:06:45 | epoch: 0046/250, training time: 934.7s, inference time: 42.2s
train loss: 24.5766, val_loss: 51.5814
2023-03-30 03:22:45 | epoch: 0047/250, training time: 917.8s, inference time: 41.5s
train loss: 24.7310, val_loss: 46.5570
2023-03-30 03:38:50 | epoch: 0048/250, training time: 923.3s, inference time: 42.2s
train loss: 24.3839, val_loss: 44.8096
2023-03-30 03:54:52 | epoch: 0049/250, training time: 919.2s, inference time: 42.8s
train loss: 24.2708, val_loss: 47.5614
2023-03-30 04:10:49 | epoch: 0050/250, training time: 914.4s, inference time: 42.5s
train loss: 24.4373, val_loss: 45.9359
2023-03-30 04:26:54 | epoch: 0051/250, training time: 922.3s, inference time: 42.6s
train loss: 24.3533, val_loss: 44.6961
2023-03-30 04:42:48 | epoch: 0052/250, training time: 911.3s, inference time: 42.7s
train loss: 24.5613, val_loss: 47.0705
2023-03-30 04:58:51 | epoch: 0053/250, training time: 917.3s, inference time: 45.7s
train loss: 24.4092, val_loss: 47.2090
2023-03-30 05:14:53 | epoch: 0054/250, training time: 920.0s, inference time: 41.7s
train loss: 24.0963, val_loss: 44.1042
2023-03-30 05:30:59 | epoch: 0055/250, training time: 924.5s, inference time: 41.5s
train loss: 24.2743, val_loss: 46.2926
2023-03-30 05:47:18 | epoch: 0056/250, training time: 936.6s, inference time: 42.7s
train loss: 24.5651, val_loss: 43.9816
2023-03-30 06:03:06 | epoch: 0057/250, training time: 906.6s, inference time: 41.2s
train loss: 23.9664, val_loss: 42.7503
val loss decrease from 43.2191 to 42.7503, saving model to ./data/GMAN.pkl
2023-03-30 06:19:00 | epoch: 0058/250, training time: 911.5s, inference time: 42.9s
train loss: 23.7744, val_loss: 46.0083
2023-03-30 06:34:56 | epoch: 0059/250, training time: 914.2s, inference time: 41.3s
train loss: 24.0984, val_loss: 43.7782
2023-03-30 06:50:58 | epoch: 0060/250, training time: 919.4s, inference time: 42.8s
train loss: 23.8763, val_loss: 45.2552
2023-03-30 07:12:11 | epoch: 0061/250, training time: 1231.8s, inference time: 41.5s
train loss: 23.7668, val_loss: 43.1186
2023-03-30 07:28:54 | epoch: 0062/250, training time: 956.3s, inference time: 46.0s
train loss: 23.7642, val_loss: 43.1620
2023-03-30 07:45:06 | epoch: 0063/250, training time: 930.7s, inference time: 41.8s
train loss: 23.7369, val_loss: 44.9465
2023-03-30 08:01:15 | epoch: 0064/250, training time: 927.1s, inference time: 41.6s
train loss: 23.7091, val_loss: 43.0698
2023-03-30 08:17:15 | epoch: 0065/250, training time: 917.8s, inference time: 41.5s
train loss: 23.7576, val_loss: 44.2744
2023-03-30 08:33:20 | epoch: 0066/250, training time: 922.6s, inference time: 42.4s
train loss: 23.6299, val_loss: 46.1291
2023-03-30 08:49:16 | epoch: 0067/250, training time: 915.5s, inference time: 41.3s
train loss: 23.4326, val_loss: 43.6992
2023-03-30 09:05:36 | epoch: 0068/250, training time: 938.0s, inference time: 42.0s
train loss: 23.6703, val_loss: 44.1533
2023-03-30 09:21:44 | epoch: 0069/250, training time: 925.8s, inference time: 41.8s
train loss: 23.4225, val_loss: 43.9711
2023-03-30 09:37:44 | epoch: 0070/250, training time: 917.3s, inference time: 43.2s
train loss: 23.3577, val_loss: 43.0222
2023-03-30 09:53:49 | epoch: 0071/250, training time: 922.7s, inference time: 42.1s
train loss: 23.5296, val_loss: 45.0265
2023-03-30 10:11:13 | epoch: 0072/250, training time: 1001.8s, inference time: 41.7s
train loss: 23.4773, val_loss: 44.4538
2023-03-30 10:27:19 | epoch: 0073/250, training time: 923.8s, inference time: 42.2s
train loss: 23.5333, val_loss: 44.3880
2023-03-30 10:43:09 | epoch: 0074/250, training time: 907.9s, inference time: 42.3s
train loss: 23.6307, val_loss: 45.3964
2023-03-30 10:59:15 | epoch: 0075/250, training time: 923.6s, inference time: 42.3s
train loss: 23.7228, val_loss: 45.8294
2023-03-30 11:15:13 | epoch: 0076/250, training time: 916.3s, inference time: 41.7s
train loss: 23.4667, val_loss: 43.0432
2023-03-30 11:30:59 | epoch: 0077/250, training time: 904.3s, inference time: 42.1s
train loss: 23.4006, val_loss: 44.9024
2023-03-30 11:47:06 | epoch: 0078/250, training time: 924.0s, inference time: 42.7s
train loss: 23.3615, val_loss: 41.1542
val loss decrease from 42.7503 to 41.1542, saving model to ./data/GMAN.pkl
2023-03-30 12:03:29 | epoch: 0079/250, training time: 939.9s, inference time: 42.4s
train loss: 23.7169, val_loss: 43.4193
2023-03-30 12:19:25 | epoch: 0080/250, training time: 914.6s, inference time: 42.0s
train loss: 23.3647, val_loss: 43.0712
2023-03-30 12:35:22 | epoch: 0081/250, training time: 915.5s, inference time: 41.3s
train loss: 23.3094, val_loss: 41.8807
2023-03-30 12:51:27 | epoch: 0082/250, training time: 922.8s, inference time: 42.0s
train loss: 23.2869, val_loss: 42.4512
2023-03-30 13:07:44 | epoch: 0083/250, training time: 934.3s, inference time: 43.0s
train loss: 23.2737, val_loss: 42.2051
2023-03-30 13:23:51 | epoch: 0084/250, training time: 923.0s, inference time: 43.8s
train loss: 23.5788, val_loss: 42.0957
2023-03-30 13:39:53 | epoch: 0085/250, training time: 920.0s, inference time: 42.3s
train loss: 23.2114, val_loss: 42.7194
2023-03-30 13:55:41 | epoch: 0086/250, training time: 906.4s, inference time: 41.2s
train loss: 23.1697, val_loss: 42.1230
2023-03-30 14:12:02 | epoch: 0087/250, training time: 939.1s, inference time: 41.6s
train loss: 23.1420, val_loss: 42.7291
2023-03-30 14:28:05 | epoch: 0088/250, training time: 921.0s, inference time: 42.6s
train loss: 23.0182, val_loss: 43.1451
2023-03-30 14:44:08 | epoch: 0089/250, training time: 920.3s, inference time: 42.1s
train loss: 23.1956, val_loss: 41.8518
2023-03-30 15:00:03 | epoch: 0090/250, training time: 913.6s, inference time: 41.3s
train loss: 23.2555, val_loss: 42.2899
2023-03-30 15:16:04 | epoch: 0091/250, training time: 919.9s, inference time: 41.6s
train loss: 23.3509, val_loss: 42.6901
2023-03-30 15:32:16 | epoch: 0092/250, training time: 929.1s, inference time: 42.5s
train loss: 22.9666, val_loss: 42.2267
2023-03-30 15:48:16 | epoch: 0093/250, training time: 917.6s, inference time: 42.5s
train loss: 22.9710, val_loss: 42.5291
2023-03-30 16:04:28 | epoch: 0094/250, training time: 930.3s, inference time: 42.3s
train loss: 22.9063, val_loss: 43.4103
2023-03-30 16:20:25 | epoch: 0095/250, training time: 915.2s, inference time: 41.7s
train loss: 22.9922, val_loss: 41.2214
2023-03-30 16:36:28 | epoch: 0096/250, training time: 920.6s, inference time: 42.1s
train loss: 23.0557, val_loss: 42.3201
2023-03-30 16:52:23 | epoch: 0097/250, training time: 911.9s, inference time: 43.2s
train loss: 23.0232, val_loss: 42.2331
2023-03-30 17:08:14 | epoch: 0098/250, training time: 909.2s, inference time: 41.1s
train loss: 22.9855, val_loss: 43.9567
2023-03-30 17:24:11 | epoch: 0099/250, training time: 916.4s, inference time: 41.4s
train loss: 22.9741, val_loss: 42.0981
2023-03-30 17:40:08 | epoch: 0100/250, training time: 914.8s, inference time: 42.1s
train loss: 22.8718, val_loss: 43.3541
2023-03-30 17:56:08 | epoch: 0101/250, training time: 918.2s, inference time: 41.8s
train loss: 22.8517, val_loss: 42.6045
2023-03-30 18:12:12 | epoch: 0102/250, training time: 921.2s, inference time: 42.3s
train loss: 22.9794, val_loss: 43.4097
2023-03-30 18:28:16 | epoch: 0103/250, training time: 921.3s, inference time: 42.6s
train loss: 22.9280, val_loss: 41.7005
2023-03-30 18:44:18 | epoch: 0104/250, training time: 920.4s, inference time: 42.0s
train loss: 22.9505, val_loss: 42.9398
2023-03-30 19:00:21 | epoch: 0105/250, training time: 919.9s, inference time: 42.4s
train loss: 23.0493, val_loss: 42.2949
2023-03-30 19:16:23 | epoch: 0106/250, training time: 920.6s, inference time: 41.3s
train loss: 22.8971, val_loss: 42.5985
2023-03-30 19:32:19 | epoch: 0107/250, training time: 913.5s, inference time: 43.1s
train loss: 22.7683, val_loss: 42.7738
2023-03-30 19:48:28 | epoch: 0108/250, training time: 927.0s, inference time: 41.6s
train loss: 22.7980, val_loss: 42.9275
2023-03-30 20:04:37 | epoch: 0109/250, training time: 927.6s, inference time: 41.2s
train loss: 22.7665, val_loss: 41.5800
2023-03-30 20:20:50 | epoch: 0110/250, training time: 931.5s, inference time: 42.1s
train loss: 22.7239, val_loss: 41.7203
2023-03-30 20:36:53 | epoch: 0111/250, training time: 920.0s, inference time: 42.1s
train loss: 22.7473, val_loss: 42.5044
2023-03-30 20:52:51 | epoch: 0112/250, training time: 917.2s, inference time: 41.4s
train loss: 22.8029, val_loss: 41.3988
2023-03-30 21:09:03 | epoch: 0113/250, training time: 928.8s, inference time: 42.6s
train loss: 22.5850, val_loss: 41.9355
2023-03-30 21:25:12 | epoch: 0114/250, training time: 927.6s, inference time: 41.3s
train loss: 22.6493, val_loss: 42.7477
2023-03-30 21:42:10 | epoch: 0115/250, training time: 975.9s, inference time: 42.4s
train loss: 22.9561, val_loss: 43.2153
2023-03-30 21:58:20 | epoch: 0116/250, training time: 927.1s, inference time: 42.7s
train loss: 22.5693, val_loss: 42.5716
2023-03-30 22:14:31 | epoch: 0117/250, training time: 930.1s, inference time: 41.4s
train loss: 22.7028, val_loss: 43.9703
2023-03-30 22:30:40 | epoch: 0118/250, training time: 927.5s, inference time: 41.7s
train loss: 22.9821, val_loss: 42.4265
2023-03-30 22:47:00 | epoch: 0119/250, training time: 937.5s, inference time: 41.7s
train loss: 22.7431, val_loss: 41.8205
2023-03-30 23:03:26 | epoch: 0120/250, training time: 943.4s, inference time: 43.1s
train loss: 22.7207, val_loss: 42.0417
2023-03-30 23:19:41 | epoch: 0121/250, training time: 933.4s, inference time: 41.4s
train loss: 22.5888, val_loss: 41.9927
2023-03-30 23:36:03 | epoch: 0122/250, training time: 939.7s, inference time: 42.2s
train loss: 22.7115, val_loss: 42.5486
2023-03-30 23:52:25 | epoch: 0123/250, training time: 938.1s, inference time: 43.4s
train loss: 22.6587, val_loss: 42.2258
2023-03-31 00:08:42 | epoch: 0124/250, training time: 935.5s, inference time: 41.7s
train loss: 22.7183, val_loss: 43.6146
2023-03-31 00:25:06 | epoch: 0125/250, training time: 942.1s, inference time: 42.5s
train loss: 22.4123, val_loss: 42.2389
2023-03-31 00:41:30 | epoch: 0126/250, training time: 940.8s, inference time: 42.5s
train loss: 22.6643, val_loss: 41.9100
2023-03-31 00:57:54 | epoch: 0127/250, training time: 941.2s, inference time: 42.6s
train loss: 22.6602, val_loss: 41.8991
2023-03-31 01:14:11 | epoch: 0128/250, training time: 934.9s, inference time: 42.2s
train loss: 22.5642, val_loss: 41.9695
2023-03-31 01:30:23 | epoch: 0129/250, training time: 930.2s, inference time: 42.5s
train loss: 22.5891, val_loss: 40.8998
val loss decrease from 41.1542 to 40.8998, saving model to ./data/GMAN.pkl
2023-03-31 01:46:50 | epoch: 0130/250, training time: 945.2s, inference time: 41.6s
train loss: 22.5632, val_loss: 42.0398
2023-03-31 02:03:09 | epoch: 0131/250, training time: 936.6s, inference time: 41.8s
train loss: 22.5819, val_loss: 40.8559
val loss decrease from 40.8998 to 40.8559, saving model to ./data/GMAN.pkl
2023-03-31 02:19:11 | epoch: 0132/250, training time: 920.7s, inference time: 41.7s
train loss: 22.5805, val_loss: 42.3842
2023-03-31 02:35:50 | epoch: 0133/250, training time: 957.2s, inference time: 41.6s
train loss: 22.6405, val_loss: 42.5919
2023-03-31 02:52:04 | epoch: 0134/250, training time: 931.8s, inference time: 42.3s
train loss: 22.4944, val_loss: 42.3772
2023-03-31 03:08:35 | epoch: 0135/250, training time: 948.8s, inference time: 41.6s
train loss: 22.6489, val_loss: 42.3022
2023-03-31 03:24:52 | epoch: 0136/250, training time: 936.1s, inference time: 41.5s
train loss: 22.6383, val_loss: 42.7689
2023-03-31 03:41:06 | epoch: 0137/250, training time: 932.2s, inference time: 41.2s
train loss: 22.4881, val_loss: 41.6921
2023-03-31 03:57:31 | epoch: 0138/250, training time: 942.3s, inference time: 43.1s
train loss: 22.4311, val_loss: 42.8661
2023-03-31 04:13:52 | epoch: 0139/250, training time: 939.3s, inference time: 41.9s
train loss: 22.5891, val_loss: 42.1202
2023-03-31 04:30:13 | epoch: 0140/250, training time: 940.1s, inference time: 40.8s
train loss: 22.6656, val_loss: 42.6622
2023-03-31 04:46:36 | epoch: 0141/250, training time: 940.2s, inference time: 42.4s
train loss: 22.8167, val_loss: 42.4502
2023-03-31 05:02:47 | epoch: 0142/250, training time: 925.3s, inference time: 45.9s
train loss: 22.5292, val_loss: 42.7270
2023-03-31 05:19:12 | epoch: 0143/250, training time: 943.2s, inference time: 41.8s
train loss: 22.3440, val_loss: 41.7543
2023-03-31 05:35:39 | epoch: 0144/250, training time: 945.8s, inference time: 41.1s
train loss: 22.5480, val_loss: 41.9277
2023-03-31 05:51:58 | epoch: 0145/250, training time: 937.1s, inference time: 42.1s
train loss: 22.4036, val_loss: 41.9529
2023-03-31 06:08:30 | epoch: 0146/250, training time: 948.7s, inference time: 43.0s
train loss: 22.6040, val_loss: 42.6563
2023-03-31 06:24:59 | epoch: 0147/250, training time: 947.2s, inference time: 42.2s
train loss: 22.3445, val_loss: 42.7975
2023-03-31 06:41:30 | epoch: 0148/250, training time: 948.3s, inference time: 42.1s
train loss: 22.5048, val_loss: 42.8271
2023-03-31 06:59:25 | epoch: 0149/250, training time: 1033.8s, inference time: 41.8s
train loss: 22.6677, val_loss: 42.7497
2023-03-31 07:16:02 | epoch: 0150/250, training time: 954.6s, inference time: 42.3s
train loss: 22.5069, val_loss: 42.4098
2023-03-31 07:32:35 | epoch: 0151/250, training time: 949.9s, inference time: 42.6s
train loss: 22.3746, val_loss: 41.8828
2023-03-31 07:48:50 | epoch: 0152/250, training time: 932.1s, inference time: 42.4s
train loss: 22.5223, val_loss: 41.2966
2023-03-31 08:05:28 | epoch: 0153/250, training time: 956.7s, inference time: 42.0s
train loss: 22.2791, val_loss: 41.8702
2023-03-31 08:21:41 | epoch: 0154/250, training time: 930.8s, inference time: 42.1s
train loss: 22.4225, val_loss: 42.2416
2023-03-31 08:38:15 | epoch: 0155/250, training time: 951.4s, inference time: 42.2s
train loss: 22.2616, val_loss: 41.9361
2023-03-31 08:54:43 | epoch: 0156/250, training time: 945.5s, inference time: 42.1s
train loss: 22.5715, val_loss: 41.6760
2023-03-31 09:11:11 | epoch: 0157/250, training time: 946.0s, inference time: 42.2s
train loss: 22.2532, val_loss: 41.6527
2023-03-31 09:27:30 | epoch: 0158/250, training time: 937.5s, inference time: 42.2s
train loss: 22.4210, val_loss: 41.9555
2023-03-31 09:44:03 | epoch: 0159/250, training time: 950.3s, inference time: 42.6s
train loss: 22.3301, val_loss: 42.1498
2023-03-31 10:00:30 | epoch: 0160/250, training time: 944.0s, inference time: 42.6s
train loss: 22.4692, val_loss: 42.4941
2023-03-31 10:16:57 | epoch: 0161/250, training time: 944.3s, inference time: 42.2s
train loss: 22.2993, val_loss: 42.3579
2023-03-31 10:33:23 | epoch: 0162/250, training time: 941.1s, inference time: 45.6s
train loss: 22.2895, val_loss: 42.5294
2023-03-31 10:49:38 | epoch: 0163/250, training time: 933.5s, inference time: 41.1s
train loss: 22.3659, val_loss: 42.2743
2023-03-31 11:06:01 | epoch: 0164/250, training time: 941.3s, inference time: 41.9s
train loss: 22.3209, val_loss: 41.8505
2023-03-31 11:22:23 | epoch: 0165/250, training time: 937.7s, inference time: 44.1s
train loss: 22.3140, val_loss: 41.6089
2023-03-31 11:38:56 | epoch: 0166/250, training time: 951.2s, inference time: 41.8s
train loss: 22.2339, val_loss: 41.6083
2023-03-31 11:55:28 | epoch: 0167/250, training time: 948.6s, inference time: 42.7s
train loss: 22.4096, val_loss: 41.6206
2023-03-31 12:11:55 | epoch: 0168/250, training time: 944.7s, inference time: 42.2s
train loss: 22.4125, val_loss: 42.5774
2023-03-31 12:28:21 | epoch: 0169/250, training time: 944.8s, inference time: 41.7s
train loss: 22.4399, val_loss: 42.4171
2023-03-31 12:44:49 | epoch: 0170/250, training time: 945.9s, inference time: 41.7s
train loss: 22.3004, val_loss: 42.4400
2023-03-31 13:01:14 | epoch: 0171/250, training time: 943.8s, inference time: 41.8s
train loss: 22.3762, val_loss: 41.9908
2023-03-31 13:17:43 | epoch: 0172/250, training time: 945.4s, inference time: 43.1s
train loss: 22.4068, val_loss: 42.5222
2023-03-31 13:34:17 | epoch: 0173/250, training time: 948.2s, inference time: 46.0s
train loss: 22.2715, val_loss: 41.6055
2023-03-31 13:50:52 | epoch: 0174/250, training time: 951.5s, inference time: 43.0s
train loss: 22.4370, val_loss: 42.1714
2023-03-31 14:07:33 | epoch: 0175/250, training time: 958.4s, inference time: 42.5s
train loss: 22.3731, val_loss: 41.2210
2023-03-31 14:23:59 | epoch: 0176/250, training time: 944.5s, inference time: 41.5s
train loss: 22.1984, val_loss: 41.3639
2023-03-31 14:40:30 | epoch: 0177/250, training time: 950.0s, inference time: 41.7s
train loss: 22.2182, val_loss: 42.4512
2023-03-31 14:56:55 | epoch: 0178/250, training time: 943.0s, inference time: 41.9s
train loss: 22.3332, val_loss: 41.9249
2023-03-31 15:13:23 | epoch: 0179/250, training time: 947.0s, inference time: 41.1s
train loss: 22.3053, val_loss: 41.4271
2023-03-31 15:29:49 | epoch: 0180/250, training time: 943.9s, inference time: 41.2s
train loss: 22.4070, val_loss: 41.7127
2023-03-31 15:46:12 | epoch: 0181/250, training time: 941.3s, inference time: 42.0s
train loss: 22.1771, val_loss: 42.5422
2023-03-31 16:04:24 | epoch: 0182/250, training time: 1050.2s, inference time: 42.2s
train loss: 22.2483, val_loss: 42.1588
2023-03-31 16:20:59 | epoch: 0183/250, training time: 952.9s, inference time: 42.3s
train loss: 22.3095, val_loss: 41.4247
2023-03-31 16:45:37 | epoch: 0184/250, training time: 1385.1s, inference time: 92.1s
train loss: 22.1755, val_loss: 41.4464
2023-03-31 17:10:25 | epoch: 0185/250, training time: 1442.9s, inference time: 45.0s
train loss: 22.2761, val_loss: 41.2628
2023-03-31 17:27:59 | epoch: 0186/250, training time: 1008.2s, inference time: 46.1s
train loss: 22.1993, val_loss: 41.8980
2023-03-31 17:45:45 | epoch: 0187/250, training time: 1018.9s, inference time: 47.5s
train loss: 22.4027, val_loss: 41.5013
2023-03-31 18:03:25 | epoch: 0188/250, training time: 1013.8s, inference time: 46.3s
train loss: 22.4077, val_loss: 41.9425
2023-03-31 18:20:58 | epoch: 0189/250, training time: 1007.1s, inference time: 45.4s
train loss: 22.2290, val_loss: 41.6444
2023-03-31 18:38:49 | epoch: 0190/250, training time: 1024.6s, inference time: 46.5s
train loss: 22.1000, val_loss: 41.5357
2023-03-31 18:56:13 | epoch: 0191/250, training time: 999.9s, inference time: 44.0s
train loss: 22.4201, val_loss: 41.3746
2023-03-31 19:13:56 | epoch: 0192/250, training time: 1019.1s, inference time: 44.2s
train loss: 22.3732, val_loss: 41.3027
2023-03-31 19:31:32 | epoch: 0193/250, training time: 1009.0s, inference time: 46.3s
train loss: 22.2877, val_loss: 42.8717
2023-03-31 19:49:18 | epoch: 0194/250, training time: 1020.0s, inference time: 46.5s
train loss: 22.2234, val_loss: 41.7842
2023-03-31 20:06:54 | epoch: 0195/250, training time: 1009.2s, inference time: 47.0s
train loss: 22.2940, val_loss: 42.4698
2023-03-31 20:24:28 | epoch: 0196/250, training time: 1005.8s, inference time: 47.4s
train loss: 22.2178, val_loss: 42.4367
2023-03-31 20:42:01 | epoch: 0197/250, training time: 1007.7s, inference time: 46.0s
train loss: 22.2683, val_loss: 41.7332
2023-03-31 20:59:45 | epoch: 0198/250, training time: 1017.2s, inference time: 46.8s
train loss: 22.0514, val_loss: 42.8835
2023-03-31 21:17:22 | epoch: 0199/250, training time: 1008.5s, inference time: 48.0s
train loss: 22.1741, val_loss: 41.6706
2023-03-31 21:35:04 | epoch: 0200/250, training time: 1017.1s, inference time: 44.6s
train loss: 22.1423, val_loss: 42.3519
2023-03-31 21:52:35 | epoch: 0201/250, training time: 1004.2s, inference time: 46.8s
train loss: 22.2957, val_loss: 42.1671
2023-03-31 22:10:07 | epoch: 0202/250, training time: 1006.7s, inference time: 45.9s
train loss: 22.1938, val_loss: 41.8570
2023-03-31 22:27:45 | epoch: 0203/250, training time: 1011.7s, inference time: 46.0s
train loss: 22.1708, val_loss: 41.9413
2023-03-31 22:45:32 | epoch: 0204/250, training time: 1019.4s, inference time: 47.8s
train loss: 22.1366, val_loss: 41.5969
2023-03-31 23:03:06 | epoch: 0205/250, training time: 1008.3s, inference time: 45.7s
train loss: 22.2847, val_loss: 42.4979
2023-03-31 23:20:38 | epoch: 0206/250, training time: 1004.1s, inference time: 47.5s
train loss: 22.2014, val_loss: 41.3284
2023-03-31 23:38:20 | epoch: 0207/250, training time: 1017.4s, inference time: 45.0s
train loss: 22.2803, val_loss: 41.5410
2023-03-31 23:55:58 | epoch: 0208/250, training time: 1010.7s, inference time: 47.3s
train loss: 22.2599, val_loss: 41.3317
2023-04-01 00:13:35 | epoch: 0209/250, training time: 1009.8s, inference time: 46.5s
train loss: 22.0842, val_loss: 41.7910
2023-04-01 00:31:11 | epoch: 0210/250, training time: 1010.1s, inference time: 46.3s
train loss: 22.1343, val_loss: 41.7244
2023-04-01 00:48:52 | epoch: 0211/250, training time: 1013.9s, inference time: 46.6s
train loss: 22.0885, val_loss: 41.6567
2023-04-01 01:06:28 | epoch: 0212/250, training time: 1010.5s, inference time: 46.1s
train loss: 22.0907, val_loss: 41.3640
2023-04-01 01:23:59 | epoch: 0213/250, training time: 1005.3s, inference time: 45.5s
train loss: 22.1437, val_loss: 41.9855
2023-04-01 01:41:32 | epoch: 0214/250, training time: 1008.0s, inference time: 45.4s
train loss: 22.1017, val_loss: 42.0002
2023-04-01 01:59:02 | epoch: 0215/250, training time: 1003.9s, inference time: 45.3s
train loss: 22.1711, val_loss: 41.3811
2023-04-01 02:16:34 | epoch: 0216/250, training time: 1005.6s, inference time: 47.0s
train loss: 22.1242, val_loss: 42.1501
2023-04-01 02:34:11 | epoch: 0217/250, training time: 1007.1s, inference time: 49.7s
train loss: 22.1524, val_loss: 41.6600
2023-04-01 02:51:39 | epoch: 0218/250, training time: 1000.6s, inference time: 47.4s
train loss: 22.2539, val_loss: 41.7367
2023-04-01 03:10:54 | epoch: 0219/250, training time: 1109.5s, inference time: 44.9s
train loss: 22.2282, val_loss: 41.8851
2023-04-01 03:28:23 | epoch: 0220/250, training time: 1002.7s, inference time: 47.0s
train loss: 22.3433, val_loss: 41.7865
2023-04-01 03:46:12 | epoch: 0221/250, training time: 1023.0s, inference time: 45.6s
train loss: 22.2020, val_loss: 41.4176
2023-04-01 04:03:48 | epoch: 0222/250, training time: 1007.4s, inference time: 48.2s
train loss: 22.2245, val_loss: 42.1702
2023-04-01 04:21:15 | epoch: 0223/250, training time: 1001.5s, inference time: 45.8s
train loss: 22.0958, val_loss: 42.3595
2023-04-01 04:39:00 | epoch: 0224/250, training time: 1017.8s, inference time: 46.9s
train loss: 22.2822, val_loss: 42.8012
2023-04-01 04:56:38 | epoch: 0225/250, training time: 1010.5s, inference time: 47.5s
train loss: 22.2955, val_loss: 42.3783
2023-04-01 05:14:13 | epoch: 0226/250, training time: 1009.6s, inference time: 45.9s
train loss: 22.2465, val_loss: 42.9043
2023-04-01 05:31:56 | epoch: 0227/250, training time: 1015.3s, inference time: 46.9s
train loss: 22.3224, val_loss: 42.7134
2023-04-01 05:49:39 | epoch: 0228/250, training time: 1017.8s, inference time: 45.6s
train loss: 22.3120, val_loss: 42.1430
2023-04-01 06:07:25 | epoch: 0229/250, training time: 1020.1s, inference time: 46.1s
train loss: 22.1990, val_loss: 42.0666
2023-04-01 06:25:23 | epoch: 0230/250, training time: 1011.7s, inference time: 66.0s
train loss: 22.0454, val_loss: 42.2941
2023-04-01 06:43:00 | epoch: 0231/250, training time: 1010.0s, inference time: 46.9s
train loss: 22.1203, val_loss: 42.2627
early stop at epoch: 0231
Training and validation are completed, and model has been stored as ./data/GMAN.pkl
**** testing model ****
loading model from ./data/GMAN.pkl
model restored!
evaluating...
testing time: 90.9s
                MAE		RMSE		MAPE
train            2.23		4.53		5.80%
val              3.16		6.50		9.48%
test             4.14		7.77		9.27%
performance in each prediction step
step: 01         3.91		7.32		8.72%
step: 02         3.93		7.37		8.77%
step: 03         3.95		7.43		8.83%
step: 04         3.98		7.50		8.90%
step: 05         4.02		7.58		9.00%
step: 06         4.07		7.66		9.11%
step: 07         4.12		7.76		9.24%
step: 08         4.19		7.86		9.38%
step: 09         4.26		7.97		9.54%
step: 10         4.34		8.09		9.72%
step: 11         4.43		8.22		9.92%
step: 12         4.53		8.36		10.12%
average:         4.14		7.76		9.27%
total time: 3844.8min

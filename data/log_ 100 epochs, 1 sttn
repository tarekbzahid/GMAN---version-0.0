K=8, L=1, SE_file='./data/SE(LAS-28).txt', batch_size=32, d=8, decay_epoch=20, learning_rate=0.01, log_file='./data/log', max_epoch=100, model_file='./data/GMAN.pkl', num_his=12, num_pred=12, patience=100, test_ratio=0.2, time_slot=5, traffic_file='./data/speed2018modified.h5', train_ratio=0.7, val_ratio=0.1
loading data...
trainX: torch.Size([24504, 12, 28])		 trainY: torch.Size([24504, 12, 28])
valX:   torch.Size([3481, 12, 28])		valY:   torch.Size([3481, 12, 28])
testX:   torch.Size([6985, 12, 28])		testY:   torch.Size([6985, 12, 28])
mean:   61.3232		std:   14.4641
data loaded!
compiling model...
trainable parameters: 209,923
**** training model ****
2023-03-23 12:04:25 | epoch: 0001/100, training time: 407.1s, inference time: 16.6s
train loss: 52.1561, val_loss: 140.8415
val loss decrease from inf to 140.8415, saving model to ./data/GMAN.pkl
2023-03-23 12:10:48 | epoch: 0002/100, training time: 367.0s, inference time: 16.3s
train loss: 42.4497, val_loss: 124.7620
val loss decrease from 140.8415 to 124.7620, saving model to ./data/GMAN.pkl
2023-03-23 12:18:21 | epoch: 0003/100, training time: 436.0s, inference time: 16.6s
train loss: 38.8893, val_loss: 78.1695
val loss decrease from 124.7620 to 78.1695, saving model to ./data/GMAN.pkl
2023-03-23 12:25:45 | epoch: 0004/100, training time: 427.0s, inference time: 16.7s
train loss: 37.6593, val_loss: 117.3437
2023-03-23 12:32:12 | epoch: 0005/100, training time: 369.5s, inference time: 17.5s
train loss: 36.4253, val_loss: 82.2796
2023-03-23 12:39:14 | epoch: 0006/100, training time: 400.7s, inference time: 21.6s
train loss: 35.7592, val_loss: 122.7040
2023-03-23 12:45:47 | epoch: 0007/100, training time: 375.9s, inference time: 16.9s
train loss: 34.9925, val_loss: 97.8979
2023-03-23 12:52:12 | epoch: 0008/100, training time: 367.9s, inference time: 16.8s
train loss: 34.4139, val_loss: 270.0100
2023-03-23 12:58:41 | epoch: 0009/100, training time: 372.4s, inference time: 16.7s
train loss: 33.6475, val_loss: 182.0767
2023-03-23 13:05:01 | epoch: 0010/100, training time: 363.2s, inference time: 17.1s
train loss: 32.9197, val_loss: 147.0420
2023-03-23 13:11:13 | epoch: 0011/100, training time: 355.1s, inference time: 17.3s
train loss: 32.3174, val_loss: 83.7801
2023-03-23 13:17:17 | epoch: 0012/100, training time: 345.7s, inference time: 17.4s
train loss: 32.1302, val_loss: 85.7636
2023-03-23 13:23:32 | epoch: 0013/100, training time: 357.2s, inference time: 18.4s
train loss: 31.8155, val_loss: 101.3492
2023-03-23 13:29:51 | epoch: 0014/100, training time: 361.5s, inference time: 16.9s
train loss: 31.2965, val_loss: 81.2167
2023-03-23 13:36:07 | epoch: 0015/100, training time: 359.2s, inference time: 16.9s
train loss: 30.9339, val_loss: 69.7920
val loss decrease from 78.1695 to 69.7920, saving model to ./data/GMAN.pkl
2023-03-23 13:42:23 | epoch: 0016/100, training time: 359.9s, inference time: 16.7s
train loss: 30.6761, val_loss: 97.1136
2023-03-23 13:48:43 | epoch: 0017/100, training time: 363.2s, inference time: 16.4s
train loss: 30.2575, val_loss: 89.0178
2023-03-23 13:55:17 | epoch: 0018/100, training time: 377.6s, inference time: 16.5s
train loss: 29.9001, val_loss: 65.3372
val loss decrease from 69.7920 to 65.3372, saving model to ./data/GMAN.pkl
2023-03-23 14:01:35 | epoch: 0019/100, training time: 361.0s, inference time: 16.5s
train loss: 29.6360, val_loss: 91.3570
2023-03-23 14:07:59 | epoch: 0020/100, training time: 367.2s, inference time: 17.1s
train loss: 29.4846, val_loss: 71.2072
2023-03-23 14:14:22 | epoch: 0021/100, training time: 365.9s, inference time: 16.9s
train loss: 28.8689, val_loss: 87.3221
2023-03-23 14:20:45 | epoch: 0022/100, training time: 365.4s, inference time: 17.3s
train loss: 28.5997, val_loss: 53.0809
val loss decrease from 65.3372 to 53.0809, saving model to ./data/GMAN.pkl
2023-03-23 14:27:11 | epoch: 0023/100, training time: 368.9s, inference time: 17.7s
train loss: 28.4604, val_loss: 71.4256
2023-03-23 14:33:26 | epoch: 0024/100, training time: 358.1s, inference time: 16.9s
train loss: 28.3666, val_loss: 81.4058
2023-03-23 14:39:47 | epoch: 0025/100, training time: 363.8s, inference time: 17.1s
train loss: 28.2541, val_loss: 95.3479
2023-03-23 14:46:11 | epoch: 0026/100, training time: 366.6s, inference time: 17.2s
train loss: 27.9196, val_loss: 91.0702
2023-03-23 14:52:28 | epoch: 0027/100, training time: 360.2s, inference time: 16.9s
train loss: 27.7263, val_loss: 83.9492
2023-03-23 14:58:56 | epoch: 0028/100, training time: 371.1s, inference time: 17.0s
train loss: 27.8503, val_loss: 74.5508
2023-03-23 15:05:20 | epoch: 0029/100, training time: 366.8s, inference time: 16.7s
train loss: 27.5991, val_loss: 61.7982
2023-03-23 15:11:37 | epoch: 0030/100, training time: 360.4s, inference time: 16.7s
train loss: 27.0130, val_loss: 59.9108
2023-03-23 15:17:58 | epoch: 0031/100, training time: 364.1s, inference time: 16.8s
train loss: 27.4010, val_loss: 64.4179
2023-03-23 15:24:14 | epoch: 0032/100, training time: 359.8s, inference time: 16.6s
train loss: 26.9208, val_loss: 75.5225
2023-03-23 15:30:48 | epoch: 0033/100, training time: 376.6s, inference time: 16.8s
train loss: 26.9552, val_loss: 84.1520
2023-03-23 15:37:09 | epoch: 0034/100, training time: 364.2s, inference time: 16.9s
train loss: 27.1234, val_loss: 62.4469
2023-03-23 15:43:34 | epoch: 0035/100, training time: 368.1s, inference time: 16.9s
train loss: 27.0758, val_loss: 76.1324
2023-03-23 15:49:55 | epoch: 0036/100, training time: 363.2s, inference time: 17.4s
train loss: 26.4882, val_loss: 103.5666
2023-03-23 15:56:19 | epoch: 0037/100, training time: 367.6s, inference time: 17.0s
train loss: 26.6240, val_loss: 74.8350
2023-03-23 16:02:38 | epoch: 0038/100, training time: 362.3s, inference time: 16.7s
train loss: 26.6942, val_loss: 59.8589
2023-03-23 16:08:58 | epoch: 0039/100, training time: 361.9s, inference time: 17.6s
train loss: 26.3548, val_loss: 83.3072
2023-03-23 16:15:19 | epoch: 0040/100, training time: 363.8s, inference time: 17.0s
train loss: 26.4355, val_loss: 51.8561
val loss decrease from 53.0809 to 51.8561, saving model to ./data/GMAN.pkl
2023-03-23 16:21:40 | epoch: 0041/100, training time: 365.1s, inference time: 16.6s
train loss: 26.0972, val_loss: 67.8982
2023-03-23 16:28:07 | epoch: 0042/100, training time: 369.8s, inference time: 17.2s
train loss: 25.8882, val_loss: 59.5983
2023-03-23 16:34:30 | epoch: 0043/100, training time: 365.1s, inference time: 17.2s
train loss: 25.8202, val_loss: 59.8112
2023-03-23 16:42:09 | epoch: 0044/100, training time: 441.5s, inference time: 17.3s
train loss: 25.9117, val_loss: 86.0701
2023-03-23 16:49:03 | epoch: 0045/100, training time: 396.9s, inference time: 17.3s
train loss: 25.7185, val_loss: 64.6578
2023-03-23 16:55:29 | epoch: 0046/100, training time: 369.6s, inference time: 16.6s
train loss: 25.8639, val_loss: 88.1675
2023-03-23 17:01:50 | epoch: 0047/100, training time: 363.9s, inference time: 16.9s
train loss: 25.6282, val_loss: 57.9883
2023-03-23 17:08:31 | epoch: 0048/100, training time: 381.3s, inference time: 19.9s
train loss: 25.5707, val_loss: 54.6529
2023-03-23 17:14:53 | epoch: 0049/100, training time: 365.1s, inference time: 16.6s
train loss: 25.5296, val_loss: 53.2410
2023-03-23 17:21:14 | epoch: 0050/100, training time: 363.6s, inference time: 16.9s
train loss: 25.5359, val_loss: 55.3988
2023-03-23 17:27:37 | epoch: 0051/100, training time: 366.5s, inference time: 17.1s
train loss: 25.2801, val_loss: 68.1859
2023-03-23 17:33:59 | epoch: 0052/100, training time: 364.8s, inference time: 17.0s
train loss: 25.4041, val_loss: 61.4789
2023-03-23 17:40:22 | epoch: 0053/100, training time: 365.7s, inference time: 16.9s
train loss: 25.3783, val_loss: 82.8116
2023-03-23 17:46:44 | epoch: 0054/100, training time: 365.3s, inference time: 17.1s
train loss: 25.3001, val_loss: 50.1808
val loss decrease from 51.8561 to 50.1808, saving model to ./data/GMAN.pkl
2023-03-23 17:53:05 | epoch: 0055/100, training time: 363.7s, inference time: 16.8s
train loss: 25.4962, val_loss: 67.5811
2023-03-23 17:59:31 | epoch: 0056/100, training time: 369.7s, inference time: 17.1s
train loss: 25.2680, val_loss: 51.2309
2023-03-23 18:05:56 | epoch: 0057/100, training time: 367.4s, inference time: 16.8s
train loss: 25.1507, val_loss: 51.7461
2023-03-23 18:12:16 | epoch: 0058/100, training time: 363.0s, inference time: 16.9s
train loss: 25.1075, val_loss: 47.7037
val loss decrease from 50.1808 to 47.7037, saving model to ./data/GMAN.pkl
2023-03-23 18:18:39 | epoch: 0059/100, training time: 366.0s, inference time: 17.0s
train loss: 25.1705, val_loss: 50.4150
2023-03-23 18:25:12 | epoch: 0060/100, training time: 376.5s, inference time: 16.7s
train loss: 25.0388, val_loss: 57.7395
2023-03-23 18:31:35 | epoch: 0061/100, training time: 366.7s, inference time: 16.7s
train loss: 24.7892, val_loss: 50.3218
2023-03-23 18:37:55 | epoch: 0062/100, training time: 362.7s, inference time: 16.9s
train loss: 24.8872, val_loss: 52.3025
2023-03-23 18:44:16 | epoch: 0063/100, training time: 364.2s, inference time: 16.9s
train loss: 24.8538, val_loss: 59.0481
2023-03-23 18:50:39 | epoch: 0064/100, training time: 366.1s, inference time: 16.6s
train loss: 24.5966, val_loss: 59.9784
2023-03-23 18:57:09 | epoch: 0065/100, training time: 372.6s, inference time: 16.9s
train loss: 24.6498, val_loss: 55.8728
2023-03-23 19:03:30 | epoch: 0066/100, training time: 365.0s, inference time: 16.7s
train loss: 24.7561, val_loss: 52.1514
2023-03-23 19:09:53 | epoch: 0067/100, training time: 366.3s, inference time: 16.8s
train loss: 24.8586, val_loss: 51.2358
2023-03-23 19:16:13 | epoch: 0068/100, training time: 363.2s, inference time: 16.7s
train loss: 24.5376, val_loss: 50.1164
2023-03-23 19:22:35 | epoch: 0069/100, training time: 364.6s, inference time: 17.2s
train loss: 24.7353, val_loss: 47.6680
val loss decrease from 47.7037 to 47.6680, saving model to ./data/GMAN.pkl
2023-03-23 19:28:59 | epoch: 0070/100, training time: 367.0s, inference time: 17.2s
train loss: 24.6654, val_loss: 44.0944
val loss decrease from 47.6680 to 44.0944, saving model to ./data/GMAN.pkl
2023-03-23 19:35:19 | epoch: 0071/100, training time: 363.0s, inference time: 16.8s
train loss: 24.5022, val_loss: 47.2032
2023-03-23 19:41:38 | epoch: 0072/100, training time: 361.6s, inference time: 16.9s
train loss: 24.5469, val_loss: 49.8249
2023-03-23 19:47:59 | epoch: 0073/100, training time: 364.2s, inference time: 16.7s
train loss: 24.6402, val_loss: 50.8980
2023-03-23 19:54:34 | epoch: 0074/100, training time: 365.5s, inference time: 29.8s
train loss: 24.4949, val_loss: 46.4129
2023-03-23 20:00:59 | epoch: 0075/100, training time: 368.3s, inference time: 17.0s
train loss: 24.6251, val_loss: 57.3396
2023-03-23 20:07:19 | epoch: 0076/100, training time: 363.2s, inference time: 16.7s
train loss: 24.6865, val_loss: 47.0591
2023-03-23 20:13:37 | epoch: 0077/100, training time: 361.4s, inference time: 16.7s
train loss: 24.5575, val_loss: 47.2738
2023-03-23 20:19:57 | epoch: 0078/100, training time: 363.5s, inference time: 16.6s
train loss: 24.5458, val_loss: 61.6659
2023-03-23 20:26:20 | epoch: 0079/100, training time: 365.7s, inference time: 16.7s
train loss: 24.3835, val_loss: 61.3815
2023-03-23 20:32:38 | epoch: 0080/100, training time: 361.1s, inference time: 16.7s
train loss: 24.5149, val_loss: 57.8018
2023-03-23 20:38:59 | epoch: 0081/100, training time: 363.9s, inference time: 17.5s
train loss: 24.4448, val_loss: 46.1236
2023-03-23 20:45:23 | epoch: 0082/100, training time: 366.4s, inference time: 17.5s
train loss: 24.3214, val_loss: 45.2161
2023-03-23 20:51:46 | epoch: 0083/100, training time: 366.2s, inference time: 16.9s
train loss: 24.3159, val_loss: 46.6594
2023-03-23 20:58:16 | epoch: 0084/100, training time: 372.2s, inference time: 17.0s
train loss: 24.3277, val_loss: 48.0936
2023-03-23 21:04:34 | epoch: 0085/100, training time: 361.5s, inference time: 16.8s
train loss: 24.2212, val_loss: 51.8264
2023-03-23 21:10:54 | epoch: 0086/100, training time: 363.5s, inference time: 17.0s
train loss: 24.3501, val_loss: 46.5898
2023-03-23 21:17:14 | epoch: 0087/100, training time: 362.4s, inference time: 17.1s
train loss: 24.1094, val_loss: 48.8839
2023-03-23 21:23:32 | epoch: 0088/100, training time: 360.8s, inference time: 16.8s
train loss: 24.1878, val_loss: 47.9471
2023-03-23 21:29:53 | epoch: 0089/100, training time: 364.8s, inference time: 16.8s
train loss: 24.3124, val_loss: 48.3396
2023-03-23 21:36:09 | epoch: 0090/100, training time: 358.7s, inference time: 16.5s
train loss: 24.1139, val_loss: 47.6983
2023-03-23 21:42:30 | epoch: 0091/100, training time: 364.2s, inference time: 16.9s
train loss: 24.0716, val_loss: 50.8059
2023-03-23 21:48:46 | epoch: 0092/100, training time: 359.3s, inference time: 16.6s
train loss: 24.4438, val_loss: 51.4622
2023-03-23 21:55:05 | epoch: 0093/100, training time: 362.3s, inference time: 16.7s
train loss: 24.1351, val_loss: 48.8640
2023-03-23 22:01:32 | epoch: 0094/100, training time: 370.6s, inference time: 17.1s
train loss: 24.4040, val_loss: 48.3526
2023-03-23 22:07:51 | epoch: 0095/100, training time: 361.8s, inference time: 16.9s
train loss: 24.1518, val_loss: 51.5168
2023-03-23 22:14:17 | epoch: 0096/100, training time: 367.9s, inference time: 17.6s
train loss: 24.2689, val_loss: 51.8799
2023-03-23 22:20:38 | epoch: 0097/100, training time: 364.4s, inference time: 16.4s
train loss: 24.1980, val_loss: 47.0866
2023-03-23 22:27:03 | epoch: 0098/100, training time: 367.9s, inference time: 17.9s
train loss: 24.2107, val_loss: 45.3107
2023-03-23 22:33:28 | epoch: 0099/100, training time: 367.6s, inference time: 17.3s
train loss: 24.2107, val_loss: 45.1575
2023-03-23 22:39:56 | epoch: 0100/100, training time: 371.0s, inference time: 16.9s
train loss: 24.0976, val_loss: 45.7756
Training and validation are completed, and model has been stored as ./data/GMAN.pkl
**** testing model ****
loading model from ./data/GMAN.pkl
model restored!
evaluating...
testing time: 33.2s
                MAE		RMSE		MAPE
train            2.77		5.08		6.80%
val              3.50		6.77		10.13%
test             4.73		8.37		10.07%
performance in each prediction step
step: 01         4.49		8.01		9.56%
step: 02         4.53		8.05		9.62%
step: 03         4.55		8.09		9.68%
step: 04         4.58		8.14		9.75%
step: 05         4.62		8.19		9.83%
step: 06         4.66		8.26		9.93%
step: 07         4.71		8.34		10.04%
step: 08         4.77		8.42		10.16%
step: 09         4.84		8.53		10.31%
step: 10         4.92		8.65		10.47%
step: 11         5.00		8.77		10.65%
step: 12         5.09		8.90		10.84%
average:         4.73		8.36		10.07%
total time: 645.6min
